{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.1"},"colab":{"name":"14.Pandas3.ipynb","provenance":[{"file_id":"https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb","timestamp":1599251027127}],"collapsed_sections":["GjNm7b0Nycv7","b2uX1tqGycv9","Zi_Uwwu4ycwS","8-ufotH4ycwT","a2awXy3_ycwe","7xnzbus8ycwi","IMFVFdzuycwo","B8eRGXTBycwt","QPRJTDCuycw1","jC-VZQIfycw5","mS8duUYGycw8","Gh7IQC87ycxE","g6267ZG4ycxF","bCbIPkelycxN","4tcDyjNQycxS","bWhG1AzyycxY","kidf3gPdycxc","_J4iIUBy4PfP","1UqFKgQm4PRj","7snaF7by4Om4","FNZm8EuC4Npc","vithhnDw6PCu","DuL02AaH6PYZ","zrGLe_3k-8h3","0OcYDM_2_O_Y","pt2hqaTa_PKg","oq-mEzi9_Pfb"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZPZkk8zFycu2","colab_type":"text"},"source":["# Aggregation and Grouping"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"xnl0UZMJycu3","colab_type":"text"},"source":["An essential piece of analysis of large data is efficient summarization: computing aggregations like ``sum()``, ``mean()``, ``median()``, ``min()``, and ``max()``, in which a single number gives insight into the nature of a potentially large dataset.\n","In this section, we'll explore aggregations in Pandas, from simple operations akin to what we've seen on NumPy arrays, to more sophisticated operations based on the concept of a ``groupby``."]},{"cell_type":"markdown","metadata":{"id":"2wdRT5ZTycu8","colab_type":"text"},"source":["For convenience, we'll use the same ``display`` magic function that we've seen previously:"]},{"cell_type":"code","metadata":{"id":"nqELQWlyycu-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","from IPython.core.display import HTML\n","\n","def display(*args):  \n","  template = \"\"\"<div style=\"float: left; padding: 10px;\">\n","  <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n","  </div>\"\"\"\n","     \n","  return HTML('\\n'.join(template.format(a, eval(a)._repr_html_())\n","                    for a in args))    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8wO6XS-ycvH","colab_type":"text"},"source":["## Planets Data\n","\n","Here we will use the Planets dataset, available via the [Seaborn package](http://seaborn.pydata.org/) (see [Visualization With Seaborn](04.14-Visualization-With-Seaborn.ipynb)).\n","It gives information on planets that astronomers have discovered around other stars (known as *extrasolar planets* or *exoplanets* for short). It can be imported with a simple Seaborn command:"]},{"cell_type":"code","metadata":{"id":"yCbzNZtYycvI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1599831171309,"user_tz":-240,"elapsed":1578,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"830120ac-1ca2-4742-eff2-84b6490d6dd1"},"source":["import seaborn as sns\n","planets = sns.load_dataset('planets')\n","planets.shape"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(1035, 6)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"FDL2cej7ycvQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1599831190338,"user_tz":-240,"elapsed":865,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"79a6512b-d085-4ee3-f10e-4818163cffe4"},"source":["planets.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>method</th>\n","      <th>number</th>\n","      <th>orbital_period</th>\n","      <th>mass</th>\n","      <th>distance</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>269.300</td>\n","      <td>7.10</td>\n","      <td>77.40</td>\n","      <td>2006</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>874.774</td>\n","      <td>2.21</td>\n","      <td>56.95</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>763.000</td>\n","      <td>2.60</td>\n","      <td>19.84</td>\n","      <td>2011</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>326.030</td>\n","      <td>19.40</td>\n","      <td>110.62</td>\n","      <td>2007</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>516.220</td>\n","      <td>10.50</td>\n","      <td>119.47</td>\n","      <td>2009</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            method  number  orbital_period   mass  distance  year\n","0  Radial Velocity       1         269.300   7.10     77.40  2006\n","1  Radial Velocity       1         874.774   2.21     56.95  2008\n","2  Radial Velocity       1         763.000   2.60     19.84  2011\n","3  Radial Velocity       1         326.030  19.40    110.62  2007\n","4  Radial Velocity       1         516.220  10.50    119.47  2009"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"iWoL7wkAycvV","colab_type":"text"},"source":["This has some details on the 1,000+ extrasolar planets discovered up to 2014."]},{"cell_type":"markdown","metadata":{"id":"OA4Cs5UvycvW","colab_type":"text"},"source":["## Simple Aggregation in Pandas"]},{"cell_type":"markdown","metadata":{"id":"742W9u0jycvW","colab_type":"text"},"source":["Earlier, we explored some of the data aggregations available for NumPy arrays.\n","As with a one-dimensional NumPy array, for a Pandas ``Series`` the aggregates return a single value:"]},{"cell_type":"code","metadata":{"id":"Q8XYuEFpycvX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1599831242310,"user_tz":-240,"elapsed":865,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"8f6690c2-0996-4f42-a782-40a1ea363269"},"source":["rng = np.random.RandomState(42)\n","ser = pd.Series(rng.rand(5))\n","ser"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.374540\n","1    0.950714\n","2    0.731994\n","3    0.598658\n","4    0.156019\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"bfzU4E35ycvd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599831250992,"user_tz":-240,"elapsed":805,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"3c7c8621-7855-42cb-c1e2-1937bb9bb3e9"},"source":["ser.sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.811925491708157"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Knv9u8ijycvh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599831262882,"user_tz":-240,"elapsed":714,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"769b758c-d045-4778-efd3-b7668684c36f"},"source":["ser.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5623850983416314"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"8ZwdfbYqycvm","colab_type":"text"},"source":["For a ``DataFrame``, by default the aggregates return results within each column:"]},{"cell_type":"code","metadata":{"id":"AZcH_Idqycvn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1599831268114,"user_tz":-240,"elapsed":803,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"e4730dff-9c5f-4bd5-c316-da797b3f45c7"},"source":["df = pd.DataFrame({'A': rng.rand(5),\n","                   'B': rng.rand(5)})\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A</th>\n","      <th>B</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.155995</td>\n","      <td>0.020584</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.058084</td>\n","      <td>0.969910</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.866176</td>\n","      <td>0.832443</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.601115</td>\n","      <td>0.212339</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.708073</td>\n","      <td>0.181825</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          A         B\n","0  0.155995  0.020584\n","1  0.058084  0.969910\n","2  0.866176  0.832443\n","3  0.601115  0.212339\n","4  0.708073  0.181825"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"8vuieChMycvq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1599831272825,"user_tz":-240,"elapsed":1042,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"b6e9f787-c785-490a-eaa4-53e3610d008d"},"source":["df.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["A    0.477888\n","B    0.443420\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"79OKpotyycvv","colab_type":"text"},"source":["By specifying the ``axis`` argument, you can instead aggregate within each row:"]},{"cell_type":"code","metadata":{"id":"1wYqetfMycvw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1599831286596,"user_tz":-240,"elapsed":798,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"c5feccbf-d8b9-4551-f719-49b89013b691"},"source":["df.mean(axis='columns')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.088290\n","1    0.513997\n","2    0.849309\n","3    0.406727\n","4    0.444949\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"fQ122B80ycv0","colab_type":"text"},"source":["Pandas ``Series`` and ``DataFrame``s include all of the common aggregates that we know; in addition, there is a convenience method ``describe()`` that computes several common aggregates for each column and returns the result.\n","Let's use this on the Planets data, for now dropping rows with missing values:"]},{"cell_type":"code","metadata":{"id":"VmMw3nDIYvoD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1599831507838,"user_tz":-240,"elapsed":1039,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"6ef244c3-0b9e-4e20-edf4-b54b6185a8a2"},"source":["planets.isna().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["method              0\n","number              0\n","orbital_period     43\n","mass              522\n","distance          227\n","year                0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"7hNGIxs2ycv1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":294},"executionInfo":{"status":"ok","timestamp":1599831556692,"user_tz":-240,"elapsed":1116,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"b47e63ee-294c-4493-e04d-0124a513fb83"},"source":["planets.dropna().describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>orbital_period</th>\n","      <th>mass</th>\n","      <th>distance</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>498.00000</td>\n","      <td>498.000000</td>\n","      <td>498.000000</td>\n","      <td>498.000000</td>\n","      <td>498.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1.73494</td>\n","      <td>835.778671</td>\n","      <td>2.509320</td>\n","      <td>52.068213</td>\n","      <td>2007.377510</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.17572</td>\n","      <td>1469.128259</td>\n","      <td>3.636274</td>\n","      <td>46.596041</td>\n","      <td>4.167284</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.00000</td>\n","      <td>1.328300</td>\n","      <td>0.003600</td>\n","      <td>1.350000</td>\n","      <td>1989.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.00000</td>\n","      <td>38.272250</td>\n","      <td>0.212500</td>\n","      <td>24.497500</td>\n","      <td>2005.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.00000</td>\n","      <td>357.000000</td>\n","      <td>1.245000</td>\n","      <td>39.940000</td>\n","      <td>2009.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2.00000</td>\n","      <td>999.600000</td>\n","      <td>2.867500</td>\n","      <td>59.332500</td>\n","      <td>2011.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>6.00000</td>\n","      <td>17337.500000</td>\n","      <td>25.000000</td>\n","      <td>354.000000</td>\n","      <td>2014.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          number  orbital_period        mass    distance         year\n","count  498.00000      498.000000  498.000000  498.000000   498.000000\n","mean     1.73494      835.778671    2.509320   52.068213  2007.377510\n","std      1.17572     1469.128259    3.636274   46.596041     4.167284\n","min      1.00000        1.328300    0.003600    1.350000  1989.000000\n","25%      1.00000       38.272250    0.212500   24.497500  2005.000000\n","50%      1.00000      357.000000    1.245000   39.940000  2009.000000\n","75%      2.00000      999.600000    2.867500   59.332500  2011.000000\n","max      6.00000    17337.500000   25.000000  354.000000  2014.000000"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"emF9LLEYycv5","colab_type":"text"},"source":["This can be a useful way to begin understanding the overall properties of a dataset.\n","For example, we see in the ``year`` column that although exoplanets were discovered as far back as 1989, half of all known expolanets were not discovered until 2010 or after.\n","This is largely thanks to the *Kepler* mission, which is a space-based telescope specifically designed for finding eclipsing planets around other stars."]},{"cell_type":"markdown","metadata":{"id":"AXp7BzeRycv6","colab_type":"text"},"source":["The following table summarizes some other built-in Pandas aggregations:\n","\n","| Aggregation              | Description                     |\n","|--------------------------|---------------------------------|\n","| ``count()``              | Total number of items           |\n","| ``first()``, ``last()``  | First and last item             |\n","| ``mean()``, ``median()`` | Mean and median                 |\n","| ``min()``, ``max()``     | Minimum and maximum             |\n","| ``std()``, ``var()``     | Standard deviation and variance |\n","| ``mad()``                | Mean absolute deviation         |\n","| ``prod()``               | Product of all items            |\n","| ``sum()``                | Sum of all items                |\n","\n","These are all methods of ``DataFrame`` and ``Series`` objects."]},{"cell_type":"markdown","metadata":{"id":"edw2PWLgycv7","colab_type":"text"},"source":["To go deeper into the data, however, simple aggregates are often not enough.\n","The next level of data summarization is the ``groupby`` operation, which allows you to quickly and efficiently compute aggregates on subsets of data."]},{"cell_type":"markdown","metadata":{"id":"GjNm7b0Nycv7","colab_type":"text"},"source":["## GroupBy: Split, Apply, Combine\n","\n","Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called ``groupby`` operation.\n","The name \"group by\" comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: *split, apply, combine*."]},{"cell_type":"markdown","metadata":{"id":"b2uX1tqGycv9","colab_type":"text"},"source":["### Split, apply, combine\n","\n","A canonical example of this split-apply-combine operation, where the \"apply\" is a summation aggregation, is illustrated in this figure:"]},{"cell_type":"markdown","metadata":{"id":"UJI-XcmXycv-","colab_type":"text"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/03.08-split-apply-combine.png?raw=1)\n"]},{"cell_type":"markdown","metadata":{"id":"20c4OYm2ycv_","colab_type":"text"},"source":["This makes clear what the ``groupby`` accomplishes:\n","\n","- The *split* step involves breaking up and grouping a ``DataFrame`` depending on the value of the specified key.\n","- The *apply* step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.\n","- The *combine* step merges the results of these operations into an output array.\n","\n","While this could certainly be done manually using some combination of the masking, aggregation, and merging commands covered earlier, an important realization is that *the intermediate splits do not need to be explicitly instantiated*. Rather, the ``GroupBy`` can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way.\n","The power of the ``GroupBy`` is that it abstracts away these steps: the user need not think about *how* the computation is done under the hood, but rather thinks about the *operation as a whole*.\n","\n","As a concrete example, let's take a look at using Pandas for the computation shown in this diagram.\n","We'll start by creating the input ``DataFrame``:"]},{"cell_type":"code","metadata":{"id":"4NvB_F1AycwB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1599831862514,"user_tz":-240,"elapsed":837,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"d9f71a16-9773-46f9-fc53-fe95e3a8ff59"},"source":["df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n","                   'data': range(6)}, columns=['key', 'data'])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>data</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>C</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  key  data\n","0   A     0\n","1   B     1\n","2   C     2\n","3   A     3\n","4   B     4\n","5   C     5"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"4dP7yQJPycwF","colab_type":"text"},"source":["The most basic split-apply-combine operation can be computed with the ``groupby()`` method of ``DataFrame``s, passing the name of the desired key column:"]},{"cell_type":"code","metadata":{"id":"h4IoNdpQycwH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599831877309,"user_tz":-240,"elapsed":868,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"93f0d6e7-2b3b-439f-c084-3fab3ee66668"},"source":["df.groupby('key')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f269b612ac8>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"Ae9_ZS3HycwK","colab_type":"text"},"source":["Notice that what is returned is not a set of ``DataFrame``s, but a ``DataFrameGroupBy`` object.\n","This object is where the magic is: you can think of it as a special view of the ``DataFrame``, which is poised to dig into the groups but does no actual computation until the aggregation is applied.\n","This \"lazy evaluation\" approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user.\n","\n","To produce a result, we can apply an aggregate to this ``DataFrameGroupBy`` object, which will perform the appropriate apply/combine steps to produce the desired result:"]},{"cell_type":"code","metadata":{"id":"9IxPOjyxycwL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"status":"ok","timestamp":1599831932129,"user_tz":-240,"elapsed":818,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"110b3245-207c-489f-cf7a-1347836839c3"},"source":["# df.groupby('key').sum()\n","df.groupby('key').mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data</th>\n","    </tr>\n","    <tr>\n","      <th>key</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>A</th>\n","      <td>1.5</td>\n","    </tr>\n","    <tr>\n","      <th>B</th>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>C</th>\n","      <td>3.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     data\n","key      \n","A     1.5\n","B     2.5\n","C     3.5"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"154KYsdkycwR","colab_type":"text"},"source":["The ``sum()`` method is just one possibility here; you can apply virtually any common Pandas or NumPy aggregation function, as well as virtually any valid ``DataFrame`` operation, as we will see in the following discussion."]},{"cell_type":"markdown","metadata":{"id":"Zi_Uwwu4ycwS","colab_type":"text"},"source":["### The GroupBy object\n","\n","The ``GroupBy`` object is a very flexible abstraction.\n","In many ways, you can simply treat it as if it's a collection of ``DataFrame``s, and it does the difficult things under the hood. Let's see some examples using the Planets data.\n","\n","Perhaps the most important operations made available by a ``GroupBy`` are *aggregate*, *filter*, *transform*, and *apply*.\n","We'll discuss each of these more fully shortly, but before that let's introduce some of the other functionality that can be used with the basic ``GroupBy`` operation."]},{"cell_type":"markdown","metadata":{"id":"8-ufotH4ycwT","colab_type":"text"},"source":["#### Column indexing\n","\n","The ``GroupBy`` object supports column indexing in the same way as the ``DataFrame``, and returns a modified ``GroupBy`` object.\n","For example:"]},{"cell_type":"code","metadata":{"id":"4GI3FCU2a5Ct","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"ok","timestamp":1599832039777,"user_tz":-240,"elapsed":782,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"4e4e5215-b2d2-47ee-8eb8-b5660d3bffbe"},"source":["planets"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>method</th>\n","      <th>number</th>\n","      <th>orbital_period</th>\n","      <th>mass</th>\n","      <th>distance</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>269.300000</td>\n","      <td>7.10</td>\n","      <td>77.40</td>\n","      <td>2006</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>874.774000</td>\n","      <td>2.21</td>\n","      <td>56.95</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>763.000000</td>\n","      <td>2.60</td>\n","      <td>19.84</td>\n","      <td>2011</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>326.030000</td>\n","      <td>19.40</td>\n","      <td>110.62</td>\n","      <td>2007</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Radial Velocity</td>\n","      <td>1</td>\n","      <td>516.220000</td>\n","      <td>10.50</td>\n","      <td>119.47</td>\n","      <td>2009</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1030</th>\n","      <td>Transit</td>\n","      <td>1</td>\n","      <td>3.941507</td>\n","      <td>NaN</td>\n","      <td>172.00</td>\n","      <td>2006</td>\n","    </tr>\n","    <tr>\n","      <th>1031</th>\n","      <td>Transit</td>\n","      <td>1</td>\n","      <td>2.615864</td>\n","      <td>NaN</td>\n","      <td>148.00</td>\n","      <td>2007</td>\n","    </tr>\n","    <tr>\n","      <th>1032</th>\n","      <td>Transit</td>\n","      <td>1</td>\n","      <td>3.191524</td>\n","      <td>NaN</td>\n","      <td>174.00</td>\n","      <td>2007</td>\n","    </tr>\n","    <tr>\n","      <th>1033</th>\n","      <td>Transit</td>\n","      <td>1</td>\n","      <td>4.125083</td>\n","      <td>NaN</td>\n","      <td>293.00</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>1034</th>\n","      <td>Transit</td>\n","      <td>1</td>\n","      <td>4.187757</td>\n","      <td>NaN</td>\n","      <td>260.00</td>\n","      <td>2008</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1035 rows × 6 columns</p>\n","</div>"],"text/plain":["               method  number  orbital_period   mass  distance  year\n","0     Radial Velocity       1      269.300000   7.10     77.40  2006\n","1     Radial Velocity       1      874.774000   2.21     56.95  2008\n","2     Radial Velocity       1      763.000000   2.60     19.84  2011\n","3     Radial Velocity       1      326.030000  19.40    110.62  2007\n","4     Radial Velocity       1      516.220000  10.50    119.47  2009\n","...               ...     ...             ...    ...       ...   ...\n","1030          Transit       1        3.941507    NaN    172.00  2006\n","1031          Transit       1        2.615864    NaN    148.00  2007\n","1032          Transit       1        3.191524    NaN    174.00  2007\n","1033          Transit       1        4.125083    NaN    293.00  2008\n","1034          Transit       1        4.187757    NaN    260.00  2008\n","\n","[1035 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"hCZYUPQeycwU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599831979979,"user_tz":-240,"elapsed":863,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"1908a4d7-0fd7-450e-d62d-7af9c8c4a789"},"source":["planets.groupby('method')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f269b612940>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"ct_fMPAKycwX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599831999569,"user_tz":-240,"elapsed":855,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"798a6359-634b-4cb9-f502-0404c4fb402a"},"source":["planets.groupby('method')['orbital_period']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.core.groupby.generic.SeriesGroupBy object at 0x7f269b634d30>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"k0ut6aMyycwa","colab_type":"text"},"source":["Here we've selected a particular ``Series`` group from the original ``DataFrame`` group by reference to its column name.\n","As with the ``GroupBy`` object, no computation is done until we call some aggregate on the object:"]},{"cell_type":"code","metadata":{"id":"6J3UkmDKycwa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1599832072069,"user_tz":-240,"elapsed":779,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"b69081d0-0874-4a7b-8564-bd67388faa9b"},"source":["planets.groupby('method')['orbital_period'].median()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["method\n","Astrometry                         631.180000\n","Eclipse Timing Variations         4343.500000\n","Imaging                          27500.000000\n","Microlensing                      3300.000000\n","Orbital Brightness Modulation        0.342887\n","Pulsar Timing                       66.541900\n","Pulsation Timing Variations       1170.000000\n","Radial Velocity                    360.200000\n","Transit                              5.714932\n","Transit Timing Variations           57.011000\n","Name: orbital_period, dtype: float64"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"PQxmcFsZbGBN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1599832136203,"user_tz":-240,"elapsed":1160,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"f598b690-c5e4-474e-8c84-07fd2742dbc5"},"source":["planets.groupby('method').median()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["method\n","Astrometry                         631.180000\n","Eclipse Timing Variations         4343.500000\n","Imaging                          27500.000000\n","Microlensing                      3300.000000\n","Orbital Brightness Modulation        0.342887\n","Pulsar Timing                       66.541900\n","Pulsation Timing Variations       1170.000000\n","Radial Velocity                    360.200000\n","Transit                              5.714932\n","Transit Timing Variations           57.011000\n","Name: orbital_period, dtype: float64"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"24bzjHJqycwd","colab_type":"text"},"source":["This gives an idea of the general scale of orbital periods (in days) that each method is sensitive to."]},{"cell_type":"markdown","metadata":{"id":"a2awXy3_ycwe","colab_type":"text"},"source":["#### Iteration over groups\n","\n","The ``GroupBy`` object supports direct iteration over the groups, returning each group as a ``Series`` or ``DataFrame``:"]},{"cell_type":"code","metadata":{"id":"62aNiO0tycwf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1599832166980,"user_tz":-240,"elapsed":1333,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"7681ba27-1ce6-4eb0-9baf-f06585ccd2ba"},"source":["for (method, group) in planets.groupby('method'):\n","    print(\"{0:30s} shape={1}\".format(method, group.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Astrometry                     shape=(2, 6)\n","Eclipse Timing Variations      shape=(9, 6)\n","Imaging                        shape=(38, 6)\n","Microlensing                   shape=(23, 6)\n","Orbital Brightness Modulation  shape=(3, 6)\n","Pulsar Timing                  shape=(5, 6)\n","Pulsation Timing Variations    shape=(1, 6)\n","Radial Velocity                shape=(553, 6)\n","Transit                        shape=(397, 6)\n","Transit Timing Variations      shape=(4, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZtY8D1PTycwi","colab_type":"text"},"source":["This can be useful for doing certain things manually, though it is often much faster to use the built-in ``apply`` functionality, which we will discuss momentarily."]},{"cell_type":"markdown","metadata":{"id":"7xnzbus8ycwi","colab_type":"text"},"source":["#### Dispatch methods\n","\n","Through some Python class magic, any method not explicitly implemented by the ``GroupBy`` object will be passed through and called on the groups, whether they are ``DataFrame`` or ``Series`` objects.\n","For example, you can use the ``describe()`` method of ``DataFrame``s to perform a set of aggregations that describe each group in the data:"]},{"cell_type":"code","metadata":{"id":"NkwGGYYDycwj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"executionInfo":{"status":"ok","timestamp":1599832299788,"user_tz":-240,"elapsed":804,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"60ad2149-a92d-4e1c-bffe-6214282af206"},"source":["planets.groupby('method')['year'].describe().unstack()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       method                       \n","count  Astrometry                          2.0\n","       Eclipse Timing Variations           9.0\n","       Imaging                            38.0\n","       Microlensing                       23.0\n","       Orbital Brightness Modulation       3.0\n","                                         ...  \n","max    Pulsar Timing                    2011.0\n","       Pulsation Timing Variations      2007.0\n","       Radial Velocity                  2014.0\n","       Transit                          2014.0\n","       Transit Timing Variations        2014.0\n","Length: 80, dtype: float64"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"WyRdS1phycwn","colab_type":"text"},"source":["Looking at this table helps us to better understand the data: for example, the vast majority of planets have been discovered by the Radial Velocity and Transit methods, though the latter only became common (due to new, more accurate telescopes) in the last decade.\n","The newest methods seem to be Transit Timing Variation and Orbital Brightness Modulation, which were not used to discover a new planet until 2011.\n","\n","This is just one example of the utility of dispatch methods.\n","Notice that they are applied *to each individual group*, and the results are then combined within ``GroupBy`` and returned.\n","Again, any valid ``DataFrame``/``Series`` method can be used on the corresponding ``GroupBy`` object, which allows for some very flexible and powerful operations!"]},{"cell_type":"markdown","metadata":{"id":"IMFVFdzuycwo","colab_type":"text"},"source":["### Aggregate, filter, transform, apply\n","\n","The preceding discussion focused on aggregation for the combine operation, but there are more options available.\n","In particular, ``GroupBy`` objects have ``aggregate()``, ``filter()``, ``transform()``, and ``apply()`` methods that efficiently implement a variety of useful operations before combining the grouped data.\n","\n","For the purpose of the following subsections, we'll use this ``DataFrame``:"]},{"cell_type":"code","metadata":{"id":"oB6YPwcCycwo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1599832331718,"user_tz":-240,"elapsed":1106,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"fcb70538-c7e2-4144-885e-0ae1d93113ab"},"source":["rng = np.random.RandomState(0)\n","df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n","                   'data1': range(6),\n","                   'data2': rng.randint(0, 10, 6)},\n","                   columns = ['key', 'data1', 'data2'])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","      <td>4</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>C</td>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  key  data1  data2\n","0   A      0      5\n","1   B      1      0\n","2   C      2      3\n","3   A      3      3\n","4   B      4      7\n","5   C      5      9"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"B8eRGXTBycwt","colab_type":"text"},"source":["#### Aggregation\n","\n","We're now familiar with ``GroupBy`` aggregations with ``sum()``, ``median()``, and the like, but the ``aggregate()`` method allows for even more flexibility.\n","It can take a string, a function, or a list thereof, and compute all the aggregates at once.\n","Here is a quick example combining all these:"]},{"cell_type":"code","metadata":{"id":"Y9MuWZB1ycwt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1599832356229,"user_tz":-240,"elapsed":799,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"b87f4200-de70-4242-e0fa-3d69638d1558"},"source":["df.groupby('key').aggregate(['min', np.median, max])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">data1</th>\n","      <th colspan=\"3\" halign=\"left\">data2</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>min</th>\n","      <th>median</th>\n","      <th>max</th>\n","      <th>min</th>\n","      <th>median</th>\n","      <th>max</th>\n","    </tr>\n","    <tr>\n","      <th>key</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>A</th>\n","      <td>0</td>\n","      <td>1.5</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4.0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>B</th>\n","      <td>1</td>\n","      <td>2.5</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>3.5</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>C</th>\n","      <td>2</td>\n","      <td>3.5</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>6.0</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    data1            data2           \n","      min median max   min median max\n","key                                  \n","A       0    1.5   3     3    4.0   5\n","B       1    2.5   4     0    3.5   7\n","C       2    3.5   5     3    6.0   9"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"X3T-LVCaycwx","colab_type":"text"},"source":["Another useful pattern is to pass a dictionary mapping column names to operations to be applied on that column:"]},{"cell_type":"code","metadata":{"id":"qOIMih-vycwy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"status":"ok","timestamp":1599832386909,"user_tz":-240,"elapsed":703,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"ebd08627-a748-4089-e350-f41488303a78"},"source":["df.groupby('key').aggregate({'data1': 'min',\n","                             'data2': 'max'})"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","    <tr>\n","      <th>key</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>A</th>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>B</th>\n","      <td>1</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>C</th>\n","      <td>2</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     data1  data2\n","key              \n","A        0      5\n","B        1      7\n","C        2      9"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"QPRJTDCuycw1","colab_type":"text"},"source":["#### Filtering\n","\n","A filtering operation allows you to drop data based on the group properties.\n","For example, we might want to keep all groups in which the standard deviation is larger than some critical value:"]},{"cell_type":"code","metadata":{"id":"iv4md2FQycw1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1599832443795,"user_tz":-240,"elapsed":1375,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"a969348e-8638-4d3d-e2ef-f685e2fba47b"},"source":["def filter_func(x):\n","    return x['data2'].std() > 4\n","\n","display('df', \"df.groupby('key').std()\", \"df.groupby('key').filter(filter_func)\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div style=\"float: left; padding: 10px;\">\n","  <p style='font-family:\"Courier New\", Courier, monospace'>df</p><div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","      <td>4</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>C</td>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","  </div>\n","<div style=\"float: left; padding: 10px;\">\n","  <p style='font-family:\"Courier New\", Courier, monospace'>df.groupby('key').std()</p><div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","    <tr>\n","      <th>key</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>A</th>\n","      <td>2.12132</td>\n","      <td>1.414214</td>\n","    </tr>\n","    <tr>\n","      <th>B</th>\n","      <td>2.12132</td>\n","      <td>4.949747</td>\n","    </tr>\n","    <tr>\n","      <th>C</th>\n","      <td>2.12132</td>\n","      <td>4.242641</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","  </div>\n","<div style=\"float: left; padding: 10px;\">\n","  <p style='font-family:\"Courier New\", Courier, monospace'>df.groupby('key').filter(filter_func)</p><div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","      <td>4</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>C</td>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"T3Al54zBycw4","colab_type":"text"},"source":["The filter function should return a Boolean value specifying whether the group passes the filtering. Here because group A does not have a standard deviation greater than 4, it is dropped from the result."]},{"cell_type":"markdown","metadata":{"id":"jC-VZQIfycw5","colab_type":"text"},"source":["#### Transformation\n","\n","While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine.\n","For such a transformation, the output is the same shape as the input.\n","A common example is to center the data by subtracting the group-wise mean:"]},{"cell_type":"code","metadata":{"id":"-VKlDTJDycw5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1599832885408,"user_tz":-240,"elapsed":793,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"010f9fa5-ae07-43ef-a85a-90e00b4b6cb5"},"source":["df.groupby('key').transform(lambda x: x - x.mean())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   data1  data2\n","0      0      5\n","1      1      0\n","2      2      3\n","3      3      3\n","4      4      7\n","5      5      9"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"mS8duUYGycw8","colab_type":"text"},"source":["#### The apply() method\n","\n","The ``apply()`` method lets you apply an arbitrary function to the group results.\n","The function should take a ``DataFrame``, and return either a Pandas object (e.g., ``DataFrame``, ``Series``) or a scalar; the combine operation will be tailored to the type of output returned.\n","\n","For example, here is an ``apply()`` that normalizes the first column by the sum of the second:"]},{"cell_type":"code","metadata":{"id":"9cIez2A5ycw9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1599832610779,"user_tz":-240,"elapsed":1187,"user":{"displayName":"Nshan Potikyan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwEEDvEOwXSKYbrUPj7UT4cqoOH73Ij4jb4412=s64","userId":"01488034774237415337"}},"outputId":"0aeed90d-cd78-42dd-a980-405c3f1b6522"},"source":["def norm_by_data2(x):\n","    # x is a DataFrame of group values\n","    x['data1'] /= x['data2'].sum()\n","    return x\n","\n","display('df', \"df.groupby('key').apply(norm_by_data2)\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div style=\"float: left; padding: 10px;\">\n","  <p style='font-family:\"Courier New\", Courier, monospace'>df</p><div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","      <td>4</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>C</td>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","  </div>\n","<div style=\"float: left; padding: 10px;\">\n","  <p style='font-family:\"Courier New\", Courier, monospace'>df.groupby('key').apply(norm_by_data2)</p><div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>data1</th>\n","      <th>data2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>0.000000</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>0.142857</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C</td>\n","      <td>0.166667</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>0.375000</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","      <td>0.571429</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>C</td>\n","      <td>0.416667</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"y4rO2yGYycxD","colab_type":"text"},"source":["``apply()`` within a ``GroupBy`` is quite flexible: the only criterion is that the function takes a ``DataFrame`` and returns a Pandas object or scalar; what you do in the middle is up to you!"]},{"cell_type":"markdown","metadata":{"id":"Gh7IQC87ycxE","colab_type":"text"},"source":["### Specifying the split key\n","\n","In the simple examples presented before, we split the ``DataFrame`` on a single column name.\n","This is just one of many options by which the groups can be defined, and we'll go through some other options for group specification here."]},{"cell_type":"markdown","metadata":{"id":"g6267ZG4ycxF","colab_type":"text"},"source":["#### A list, array, series, or index providing the grouping keys\n","\n","The key can be any series or list with a length matching that of the ``DataFrame``. For example:"]},{"cell_type":"code","metadata":{"id":"qETdw2E-ycxF","colab_type":"code","colab":{}},"source":["L = [0, 1, 0, 1, 2, 0]\n","display('df', 'df.groupby(L).sum()')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fB88CkWuycxJ","colab_type":"text"},"source":["Of course, this means there's another, more verbose way of accomplishing the ``df.groupby('key')`` from before:"]},{"cell_type":"code","metadata":{"id":"ehsHo7KQycxK","colab_type":"code","colab":{}},"source":["display('df', \"df.groupby(df['key']).sum()\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCbIPkelycxN","colab_type":"text"},"source":["#### A dictionary or series mapping index to group\n","\n","Another method is to provide a dictionary that maps index values to the group keys:"]},{"cell_type":"code","metadata":{"id":"XRLQEQzbycxO","colab_type":"code","colab":{}},"source":["df2 = df.set_index('key')\n","mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'}\n","display('df2', 'df2.groupby(mapping).sum()')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4tcDyjNQycxS","colab_type":"text"},"source":["#### Any Python function\n","\n","Similar to mapping, you can pass any Python function that will input the index value and output the group:"]},{"cell_type":"code","metadata":{"id":"L1MJ1YE0ycxT","colab_type":"code","colab":{}},"source":["display('df2', 'df2.groupby(str.lower).mean()')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWhG1AzyycxY","colab_type":"text"},"source":["#### A list of valid keys\n","\n","Further, any of the preceding key choices can be combined to group on a multi-index:"]},{"cell_type":"code","metadata":{"id":"QsbY9J46ycxZ","colab_type":"code","colab":{}},"source":["df2.groupby([str.lower, mapping]).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kidf3gPdycxc","colab_type":"text"},"source":["### Grouping example\n","\n","As an example of this, in a couple lines of Python code we can put all these together and count discovered planets by method and by decade:"]},{"cell_type":"code","metadata":{"id":"8GgRNtRKycxc","colab_type":"code","colab":{}},"source":["decade = 10 * (planets['year'] // 10)\n","decade = decade.astype(str) + 's'\n","decade.name = 'decade'\n","planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-T-KHNwYycxh","colab_type":"text"},"source":["This shows the power of combining many of the operations we've discussed up to this point when looking at realistic datasets.\n","We immediately gain a coarse understanding of when and how planets have been discovered over the past several decades!\n","\n","Here I would suggest digging into these few lines of code, and evaluating the individual steps to make sure you understand exactly what they are doing to the result.\n","It's certainly a somewhat complicated example, but understanding these pieces will give you the means to similarly explore your own data."]},{"cell_type":"markdown","metadata":{"id":"_J4iIUBy4PfP","colab_type":"text"},"source":["# Pivot Tables\n","\n","We have seen how the ``GroupBy`` abstraction lets us explore relationships within a dataset.\n","A *pivot table* is a similar operation that is commonly seen in **spreadsheets** and other programs that operate on tabular data.\n","The pivot table takes simple column-wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data.\n","The difference between pivot tables and ``GroupBy`` can sometimes cause confusion; one can think of pivot tables as essentially a *multidimensional* version of ``GroupBy`` aggregation.\n","That is, you split-apply-combine, but both the split and the combine happen across not a one-dimensional index, but across a two-dimensional grid."]},{"cell_type":"markdown","metadata":{"id":"1UqFKgQm4PRj","colab_type":"text"},"source":["## Motivating Pivot Tables\n","\n","For the examples in this section, we'll use the database of passengers on the *Titanic*, available through the Seaborn library․"]},{"cell_type":"code","metadata":{"id":"bmt1r76v4fxF","colab_type":"code","colab":{}},"source":["titanic = sns.load_dataset('titanic')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IFc-9S7N4O0M","colab_type":"code","colab":{}},"source":["titanic.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7snaF7by4Om4","colab_type":"text"},"source":["This contains a wealth of information on each passenger of that ill-fated voyage, including gender, age, class, fare paid, and much more.\n","\n","## Pivot Tables by Hand\n","\n","To start learning more about this data, we might begin by grouping according to gender, survival status, or some combination thereof.\n","If you have read the previous section, you might be tempted to apply a ``GroupBy`` operation–for example, let's look at survival rate by gender:"]},{"cell_type":"code","metadata":{"id":"_gHN7_Fk4Obh","colab_type":"code","colab":{}},"source":["titanic.groupby('sex')[['survived']].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjMMGARK4OOn","colab_type":"text"},"source":["This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived!\n","\n","This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class.\n","Using the vocabulary of ``GroupBy``, we might proceed using something like this:\n","we *group by* class and gender, *select* survival, *apply* a mean aggregate, *combine* the resulting groups, and then *unstack* the hierarchical index to reveal the hidden multidimensionality. In code:"]},{"cell_type":"code","metadata":{"id":"4IXG7P8s4OCe","colab_type":"code","colab":{}},"source":["titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62uf0Dnz4N04","colab_type":"text"},"source":["This gives us a better idea of how both gender and class affected survival, but the code is starting to look a bit garbled.\n","While each step of this pipeline makes sense in light of the tools we've previously discussed, the long string of code is not particularly easy to read or use.\n","This two-dimensional ``GroupBy`` is common enough that Pandas includes a convenience routine, ``pivot_table``, which succinctly handles this type of multi-dimensional aggregation."]},{"cell_type":"markdown","metadata":{"id":"FNZm8EuC4Npc","colab_type":"text"},"source":["## Pivot Table Syntax\n","\n","Here is the equivalent to the preceding operation using the ``pivot_table`` method of ``DataFrame``s:"]},{"cell_type":"code","metadata":{"id":"bpimM6dR4Nd5","colab_type":"code","colab":{}},"source":["titanic.pivot_table('survived', index='sex', columns='class')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7aypuHfV4NQk","colab_type":"text"},"source":["This is eminently more readable than the ``groupby`` approach, and produces the same result.\n","As you might expect of an early 20th-century transatlantic cruise, the survival gradient favors both women and higher classes.\n","First-class women survived with near certainty (hi, Rose!), while only one in ten third-class men survived (sorry, Jack!)."]},{"cell_type":"markdown","metadata":{"id":"vithhnDw6PCu","colab_type":"text"},"source":["### Multi-level pivot tables\n","\n","Just as in the ``GroupBy``, the grouping in pivot tables can be specified with multiple levels, and via a number of options.\n","For example, we might be interested in looking at age as a third dimension.\n","We'll bin the age using the ``pd.cut`` function:"]},{"cell_type":"code","metadata":{"id":"HVXi2txQ6PH-","colab_type":"code","colab":{}},"source":["age = pd.cut(titanic['age'], [0, 18, 80])\n","titanic.pivot_table('survived', ['sex', age], 'class')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heZ0Yxwf6PNR","colab_type":"text"},"source":["We can apply the same strategy when working with the columns as well; let's add info on the fare paid using ``pd.qcut`` to automatically compute quantiles:"]},{"cell_type":"code","metadata":{"id":"rayDGtKf6PS1","colab_type":"code","colab":{}},"source":["fare = pd.qcut(titanic['fare'], 2)\n","titanic.pivot_table('survived', ['sex', age], [fare, 'class'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuL02AaH6PYZ","colab_type":"text"},"source":["The result is a four-dimensional aggregation with hierarchical indices, shown in a grid demonstrating the relationship between the values.\n","\n","### Additional pivot table options\n","\n","The full call signature of the ``pivot_table`` method of ``DataFrame``s is as follows:\n","\n","```python\n","# call signature as of Pandas 0.18\n","DataFrame.pivot_table(data, values=None, index=None, columns=None,\n","                      aggfunc='mean', fill_value=None, margins=False,\n","                      dropna=True, margins_name='All')\n","```\n","\n","We've already seen examples of the first three arguments; here we'll take a quick look at the remaining ones.\n","Two of the options, ``fill_value`` and ``dropna``, have to do with missing data and are fairly straightforward; we will not show examples of them here.\n","\n","The ``aggfunc`` keyword controls what type of aggregation is applied, which is a mean by default.\n","As in the GroupBy, the aggregation specification can be a string representing one of several common choices (e.g., ``'sum'``, ``'mean'``, ``'count'``, ``'min'``, ``'max'``, etc.) or a function that implements an aggregation (e.g., ``np.sum()``, ``min()``, ``sum()``, etc.).\n","Additionally, it can be specified as a dictionary mapping a column to any of the above desired options:"]},{"cell_type":"code","metadata":{"id":"uZtkJC3x6PeU","colab_type":"code","colab":{}},"source":["titanic.pivot_table(index='sex', columns='class',\n","                    aggfunc={'survived':sum, 'fare':'mean'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T161lCwG6Phy","colab_type":"text"},"source":["Notice also here that we've omitted the ``values`` keyword; when specifying a mapping for ``aggfunc``, this is determined automatically."]},{"cell_type":"markdown","metadata":{"id":"Uvx8bBCM6Pmg","colab_type":"text"},"source":["At times it's useful to compute totals along each grouping.\n","This can be done via the ``margins`` keyword:"]},{"cell_type":"code","metadata":{"id":"-dcGIamp4NCb","colab_type":"code","colab":{}},"source":["titanic.pivot_table('survived', index='sex', columns='class', margins=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hVgncwFo7Eyo","colab_type":"text"},"source":["Here this automatically gives us information about the class-agnostic survival rate by gender, the gender-agnostic survival rate by class, and the overall survival rate of 38%.\n","The margin label can be specified with the ``margins_name`` keyword, which defaults to ``\"All\"``."]},{"cell_type":"markdown","metadata":{"id":"PRen6YAu7XxA","colab_type":"text"},"source":["# Vectorized String Operations\n","\n","One strength of Python is its relative ease in handling and manipulating string data.\n","Pandas builds on this and provides a comprehensive set of *vectorized string operations* that become an essential piece of the type of munging required when working with real-world data.\n","In this section, we'll walk through some of the Pandas string operations, and then take a look at using them to partially clean up a very messy dataset of recipes collected from the Internet.\n","\n","## Introducing Pandas String Operations\n","\n","We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. For example:"]},{"cell_type":"code","metadata":{"id":"dWNNkV2O7X2w","colab_type":"code","colab":{}},"source":["x = np.array([2, 3, 5, 7, 11, 13])\n","x * 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VdOvYSye-rfW","colab_type":"text"},"source":["This *vectorization* of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done.\n","For arrays of strings, NumPy does not provide such simple access, and thus you're stuck using a more verbose loop syntax:"]},{"cell_type":"code","metadata":{"id":"42o2pyOn-rmE","colab_type":"code","colab":{}},"source":["data = ['peter', 'Paul', 'MARY', 'gUIDO']\n","[s.capitalize() for s in data]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ieM1LqWD-rsu","colab_type":"text"},"source":["This is perhaps sufficient to work with some data, but it will break if there are any missing values.\n","For example:"]},{"cell_type":"code","metadata":{"id":"q1M9Mf9X-rzW","colab_type":"code","colab":{}},"source":["data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\n","[s.capitalize() for s in data]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JW6FaW0f-r6Q","colab_type":"text"},"source":["Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the ``str`` attribute of Pandas Series and Index objects containing strings.\n","So, for example, suppose we create a Pandas Series with this data:"]},{"cell_type":"code","metadata":{"id":"GwMPksiG-r_X","colab_type":"code","colab":{}},"source":["names = pd.Series(data)\n","names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO4Ve9RH-sFa","colab_type":"text"},"source":["We can now call a single method that will capitalize all the entries, while skipping over any missing values:"]},{"cell_type":"code","metadata":{"id":"xzmMoiBr-sL0","colab_type":"code","colab":{}},"source":["names.str.capitalize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VjPuNRO-sSW","colab_type":"text"},"source":["Using tab completion on this ``str`` attribute will list all the vectorized string methods available to Pandas.\n","\n","## Tables of Pandas String Methods\n","\n","If you have a good understanding of string manipulation in Python, most of Pandas string syntax is intuitive enough that it's probably sufficient to just list a table of available methods; we will start with that here, before diving deeper into a few of the subtleties.\n","The examples in this section use the following series of names:"]},{"cell_type":"code","metadata":{"id":"mXAQHybR-8cM","colab_type":"code","colab":{}},"source":["monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\n","                   'Eric Idle', 'Terry Jones', 'Michael Palin'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrGLe_3k-8h3","colab_type":"text"},"source":["### Methods similar to Python string methods\n","Nearly all Python's built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas ``str`` methods that mirror Python string methods:\n","\n","|             |                  |                  |                  |\n","|-------------|------------------|------------------|------------------|\n","|``len()``    | ``lower()``      | ``translate()``  | ``islower()``    | \n","|``ljust()``  | ``upper()``      | ``startswith()`` | ``isupper()``    | \n","|``rjust()``  | ``find()``       | ``endswith()``   | ``isnumeric()``  | \n","|``center()`` | ``rfind()``      | ``isalnum()``    | ``isdecimal()``  | \n","|``zfill()``  | ``index()``      | ``isalpha()``    | ``split()``      | \n","|``strip()``  | ``rindex()``     | ``isdigit()``    | ``rsplit()``     | \n","|``rstrip()`` | ``capitalize()`` | ``isspace()``    | ``partition()``  | \n","|``lstrip()`` |  ``swapcase()``  |  ``istitle()``   | ``rpartition()`` |\n","\n","Notice that these have various return values. Some, like ``lower()``, return a series of strings:"]},{"cell_type":"code","metadata":{"id":"mvTl5gWk-8nS","colab_type":"code","colab":{}},"source":["monte.str.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"451qe1NI-8so","colab_type":"text"},"source":["But some others return numbers:"]},{"cell_type":"code","metadata":{"id":"GSlufxfR-8x7","colab_type":"code","colab":{}},"source":["monte.str.len()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TPmO9u0M-84G","colab_type":"text"},"source":["Or Boolean values:"]},{"cell_type":"code","metadata":{"id":"gbLcolP6-889","colab_type":"code","colab":{}},"source":["monte.str.startswith('T')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F8vAerDk-9Ce","colab_type":"text"},"source":["Still others return lists or other compound values for each element:"]},{"cell_type":"code","metadata":{"id":"bA7NNvse-9Hp","colab_type":"code","colab":{}},"source":["monte.str.split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OcYDM_2_O_Y","colab_type":"text"},"source":["We'll see further manipulations of this kind of series-of-lists object as we continue our discussion.\n","\n","### Methods using regular expressions\n","\n","In addition, there are several methods that accept regular expressions to examine the content of each string element, and follow some of the API conventions of Python's built-in ``re`` module:\n","\n","| Method | Description |\n","|--------|-------------|\n","| ``match()`` | Call ``re.match()`` on each element, returning a boolean. |\n","| ``extract()`` | Call ``re.match()`` on each element, returning matched groups as strings.|\n","| ``findall()`` | Call ``re.findall()`` on each element |\n","| ``replace()`` | Replace occurrences of pattern with some other string|\n","| ``contains()`` | Call ``re.search()`` on each element, returning a boolean |\n","| ``count()`` | Count occurrences of pattern|\n","| ``split()``   | Equivalent to ``str.split()``, but accepts regexps |\n","| ``rsplit()`` | Equivalent to ``str.rsplit()``, but accepts regexps |\n","\n","Or we can do something more complicated, like finding all names that start and end with a consonant, making use of the start-of-string (``^``) and end-of-string (``$``) regular expression characters:"]},{"cell_type":"code","metadata":{"id":"BItKn67j_PFC","colab_type":"code","colab":{}},"source":["monte.str.findall(r'^[^AEIOU].*[^aeiou]$')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pt2hqaTa_PKg","colab_type":"text"},"source":["The ability to concisely apply regular expressions across ``Series`` or ``Dataframe`` entries opens up many possibilities for analysis and cleaning of data.\n","\n","### Miscellaneous methods\n","Finally, there are some miscellaneous methods that enable other convenient operations:\n","\n","| Method | Description |\n","|--------|-------------|\n","| ``get()`` | Index each element |\n","| ``slice()`` | Slice each element|\n","| ``slice_replace()`` | Replace slice in each element with passed value|\n","| ``cat()``      | Concatenate strings|\n","| ``repeat()`` | Repeat values |\n","| ``normalize()`` | Return Unicode form of string |\n","| ``pad()`` | Add whitespace to left, right, or both sides of strings|\n","| ``wrap()`` | Split long strings into lines with length less than a given width|\n","| ``join()`` | Join strings in each element of the Series with passed separator|\n","| ``get_dummies()`` | extract dummy variables as a dataframe |\n","\n","#### Vectorized item access and slicing\n","\n","The ``get()`` and ``slice()`` operations, in particular, enable vectorized element access from each array.\n","For example, we can get a slice of the first three characters of each array using ``str.slice(0, 3)``.\n","Note that this behavior is also available through Python's normal indexing syntax–for example, ``df.str.slice(0, 3)`` is equivalent to ``df.str[0:3]``:"]},{"cell_type":"code","metadata":{"id":"5no4WsCV_PRB","colab_type":"code","colab":{}},"source":["monte.str[0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CAuwuFVs_PVf","colab_type":"text"},"source":["Indexing via ``df.str.get(i)`` and ``df.str[i]`` is likewise similar.\n","\n","These ``get()`` and ``slice()`` methods also let you access elements of arrays returned by ``split()``.\n","For example, to extract the last name of each entry, we can combine ``split()`` and ``get()``:"]},{"cell_type":"code","metadata":{"id":"8bS8Y-lR_PaH","colab_type":"code","colab":{}},"source":["monte.str.split().str.get(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oq-mEzi9_Pfb","colab_type":"text"},"source":["#### Indicator variables\n","\n","Another method that requires a bit of extra explanation is the ``get_dummies()`` method.\n","This is useful when your data has a column containing some sort of coded indicator.\n","For example, we might have a dataset that contains information in the form of codes, such as A=\"born in America,\" B=\"born in the United Kingdom,\" C=\"likes cheese,\" D=\"likes spam\":"]},{"cell_type":"code","metadata":{"id":"foT9ktuV_oqh","colab_type":"code","colab":{}},"source":["full_monte = pd.DataFrame({'name': monte,\n","                           'info': ['B|C|D', 'B|D', 'A|C',\n","                                    'B|D', 'B|C', 'B|C|D']})\n","full_monte"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BgaDf_1N_Pkq","colab_type":"text"},"source":["The ``get_dummies()`` routine lets you quickly split-out these indicator variables into a ``DataFrame``:"]},{"cell_type":"code","metadata":{"id":"J_8T81oE_PqI","colab_type":"code","colab":{}},"source":["full_monte['info'].str.get_dummies('|')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMjKU5AT_PvL","colab_type":"text"},"source":["With these operations as building blocks, you can construct an endless range of string processing procedures when cleaning your data.\n","\n","We won't dive further into these methods here, but you can learn more in the [Pandas online documentation](http://pandas.pydata.org/pandas-docs/stable/text.html)."]},{"cell_type":"markdown","metadata":{"id":"qjZs9WbE_P0i","colab_type":"text"},"source":["## Example: Recipe Database\n","\n","These vectorized string operations become most useful in the process of cleaning up messy, real-world data.\n","Here I'll walk through an example of that, using an open recipe database compiled from various sources on the Web.\n","Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand.\n","\n","The scripts used to compile this can be found at https://github.com/fictivekin/openrecipes, and the link to the current version of the database is found there as well.\n","\n","The compressed version of the database is about 30 MB, and can be downloaded from [here](https://github.com/sameergarg/scala-elasticsearch/raw/master/conf/recipeitems-latest.json.gz). We will use a smaller part of the database for the sake of example. The database can be found in ``Lab_files/Lab14`` folder."]},{"cell_type":"markdown","metadata":{"id":"t7oTLRBwAV2Z","colab_type":"text"},"source":["The database is in JSON format, so we will try ``pd.read_json`` to read it:"]},{"cell_type":"code","metadata":{"id":"6p4Li6PzAV7W","colab_type":"code","colab":{}},"source":["try:\n","    recipes = pd.read_json('recipeitems500.json')\n","except ValueError as e:\n","    print(\"ValueError:\", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlueBuO9AWAh","colab_type":"text"},"source":["We get a ``ValueError`` mentioning that there is \"trailing data.\"\n","Searching for the text of this error on the Internet, it seems that it's due to using a file in which *each line* is itself a valid JSON, but the full file is not.\n","Let's check if this interpretation is true:"]},{"cell_type":"code","metadata":{"id":"KnDL42S0AWFs","colab_type":"code","colab":{}},"source":["with open('recipeitems500.json') as f:\n","    line = f.readline()\n","print(pd.read_json(line))    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FEO1cS7xAWKd","colab_type":"text"},"source":["Yes, apparently each line is a valid JSON, so we'll need to string them together.\n","One way we can do this is to actually construct a string representation containing all these JSON entries, and then load the whole thing with ``pd.read_json``:"]},{"cell_type":"code","metadata":{"id":"zdoUMdtlAWP1","colab_type":"code","colab":{}},"source":["# read the entire file into a Python array\n","with open('recipeitems500.json', 'r') as f:\n","    # Extract each line\n","    data = (line.strip() for line in f)\n","    # Reformat so each line is the element of a list\n","    data_json = \"[{0}]\".format(','.join(data))\n","# read the result as a JSON\n","recipes = pd.read_json(data_json)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TiC1UXdibooO","colab_type":"code","colab":{}},"source":["recipes.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxNgYp16AWUw","colab_type":"text"},"source":["We see there are nearly 500 recipes, and 16 columns.\n","Let's take a look at one row to see what we have:"]},{"cell_type":"code","metadata":{"id":"E3kkMydXAWZd","colab_type":"code","colab":{}},"source":["recipes.iloc[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N59pSWfM7FbA","colab_type":"text"},"source":["There is a lot of information there, but much of it is in a very messy form, as is typical of data scraped from the Web.\n","In particular, the ingredient list is in string format; we're going to have to carefully extract the information we're interested in.\n","Let's start by taking a closer look at the ingredients:"]},{"cell_type":"code","metadata":{"id":"-Ae1iZWTfPr3","colab_type":"code","colab":{}},"source":["recipes.ingredients.str.len().describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DKw_BHw0fPxX","colab_type":"text"},"source":["The ingredient lists average 289 characters long, with a minimum of 29 and a maximum of 988 characters!\n","\n","Just out of curiousity, let's see which recipe has the longest ingredient list:"]},{"cell_type":"code","metadata":{"id":"DMPvG5fPfP15","colab_type":"code","colab":{}},"source":["recipes.name[np.argmax(recipes.ingredients.str.len())]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_Y5FYtsfP6O","colab_type":"text"},"source":["We can do other aggregate explorations; for example, let's see how many of the recipes are for breakfast food:"]},{"cell_type":"code","metadata":{"id":"HJsiV0EOfP-q","colab_type":"code","colab":{}},"source":["recipes.description.str.contains('[Bb]reakfast').sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxRXaKfHfQC2","colab_type":"text"},"source":["Or how many of the recipes list cinnamon as an ingredient:"]},{"cell_type":"code","metadata":{"id":"6p8zZyTLfQHh","colab_type":"code","colab":{}},"source":["recipes.ingredients.str.contains('[Cc]innamon').sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9olq0qkEfQLj","colab_type":"text"},"source":["We could even look to see whether any recipes misspell the ingredient as \"cinamon\":"]},{"cell_type":"code","metadata":{"id":"BrOBRmx2fQQF","colab_type":"code","colab":{}},"source":["recipes.ingredients.str.contains('[Cc]inamon').sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWQ1AgmAfQUz","colab_type":"text"},"source":["This is the type of essential data exploration that is possible with Pandas string tools.\n","It is data munging like this that Python really excels at."]},{"cell_type":"markdown","metadata":{"id":"wxMvbL2WfQZz","colab_type":"text"},"source":["### A simple recipe recommender\n","\n","Let's go a bit further, and start working on a simple recipe recommendation system: given a list of ingredients, find a recipe that uses all those ingredients.\n","While conceptually straightforward, the task is complicated by the heterogeneity of the data: there is no easy operation, for example, to extract a clean list of ingredients from each row.\n","So we will cheat a bit: we'll start with a list of common ingredients, and simply search to see whether they are in each recipe's ingredient list.\n","For simplicity, let's just stick with herbs and spices for the time being:"]},{"cell_type":"code","metadata":{"id":"310JA8HlgTUD","colab_type":"code","colab":{}},"source":["spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',\n","              'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GB6hSOAdgTJr","colab_type":"text"},"source":["We can then build a Boolean ``DataFrame`` consisting of True and False values, indicating whether this ingredient appears in the list:"]},{"cell_type":"code","metadata":{"id":"pTwgl8DkgS-_","colab_type":"code","colab":{}},"source":["import re\n","spice_df = pd.DataFrame(dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE))\n","                             for spice in spice_list))\n","spice_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-0TfFnmgS1H","colab_type":"text"},"source":["Now, as an example, let's say we'd like to find a recipe that uses parsley, paprika, and tarragon.\n","We can compute this very quickly using the ``query()`` method of ``DataFrame``s, which we will discuss soon."]},{"cell_type":"code","metadata":{"id":"s7IQL3WTgSp8","colab_type":"code","colab":{}},"source":["selection = spice_df.query('salt & oregano & pepper')\n","len(selection)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ly5jeqYgtqN","colab_type":"text"},"source":["We find only 5 recipes with this combination; let's use the index returned by this selection to discover the names of the recipes that have this combination:"]},{"cell_type":"code","metadata":{"id":"u48MZzLagtvy","colab_type":"code","colab":{}},"source":["recipes.name[selection.index]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Qkk-0gzgt1i","colab_type":"text"},"source":["Now that we have narrowed down our recipe selection by a factor of almost 20,000, we are in a position to make a more informed decision about what we'd like to cook for dinner.\n","\n","### Going further with recipes\n","\n","Hopefully this example has given you a bit of a flavor for the types of data cleaning operations that are efficiently enabled by Pandas string methods.\n","Of course, building a very robust recipe recommendation system would require a *lot* more work!\n","Extracting full ingredient lists from each recipe would be an important piece of the task; unfortunately, the wide variety of formats used makes this a relatively time-consuming process.\n","This points to the truism that in data science, cleaning and munging of real-world data often comprises the majority of the work, and Pandas provides the tools that can help you do this efficiently."]},{"cell_type":"markdown","metadata":{"id":"f4f8c6Zugt7A","colab_type":"text"},"source":["# Working with Time Series\n","\n","Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time-indexed data.\n","Date and time data comes in a few flavors, which we will discuss here:\n","\n","- *Time stamps* reference particular moments in time (e.g., July 4th, 2015 at 7:00am).\n","- *Time intervals* and *periods* reference a length of time between a particular beginning and end point; for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods comprising days).\n","- *Time deltas* or *durations* reference an exact length of time (e.g., a duration of 22.56 seconds).\n","\n","In this section, we will introduce how to work with each of these types of date/time data in Pandas.\n","This short section is by no means a complete guide to the time series tools available in Python or Pandas, but instead is intended as a broad overview of how you as a user should approach working with time series.\n","We will start with a brief discussion of tools for dealing with dates and times in Python, before moving more specifically to a discussion of the tools provided by Pandas.\n","After listing some resources that go into more depth, we will review some short examples of working with time series data in Pandas.\n","\n","## Dates and Times in Python\n","\n","The Python world has a number of available representations of dates, times, deltas, and timespans.\n","While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python.\n","\n","### Native Python dates and times: ``datetime`` and ``dateutil``\n","\n","Python's basic objects for working with dates and times reside in the built-in ``datetime`` module.\n","Along with the third-party ``dateutil`` module, you can use it to quickly perform a host of useful functionalities on dates and times.\n","For example, you can manually build a date using the ``datetime`` type:"]},{"cell_type":"code","metadata":{"id":"xMIT-NKlguAR","colab_type":"code","colab":{}},"source":["from datetime import datetime\n","datetime(year=2015, month=7, day=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBk1HpsEguFo","colab_type":"text"},"source":["Or, using the ``dateutil`` module, you can parse dates from a variety of string formats:"]},{"cell_type":"code","metadata":{"id":"sPOKSBdEguLs","colab_type":"code","colab":{}},"source":["from dateutil import parser\n","date = parser.parse(\"4th of July, 2015\")\n","date"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mgEsEq9PguQY","colab_type":"text"},"source":["Once you have a ``datetime`` object, you can do things like printing the day of the week:"]},{"cell_type":"code","metadata":{"id":"s76AQtOCguVQ","colab_type":"code","colab":{}},"source":["date.strftime('%A')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOyS8ecKmrdY","colab_type":"text"},"source":["In the final line, we've used one of the standard string format codes for printing dates (``\"%A\"``), which you can read about in the [strftime section](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) of Python's [datetime documentation](https://docs.python.org/3/library/datetime.html).\n","Documentation of other useful date utilities can be found in [dateutil's online documentation](http://labix.org/python-dateutil).\n","\n","The power of ``datetime`` and ``dateutil`` lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in.\n","Where they break down is when you wish to work with large arrays of dates and times:\n","just as lists of Python numerical variables are suboptimal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates.\n","\n","### Typed arrays of times: NumPy's ``datetime64``\n","\n","The weaknesses of Python's datetime format inspired the NumPy team to add a set of native time series data type to NumPy.\n","The ``datetime64`` dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly.\n","The ``datetime64`` requires a very specific input format:"]},{"cell_type":"code","metadata":{"id":"QCwpxp1RmuyW","colab_type":"code","colab":{}},"source":["date = np.array('2015-07-04', dtype=np.datetime64)\n","date"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aA0QYvMNmu34","colab_type":"text"},"source":["Once we have this date formatted, however, we can quickly do vectorized operations on it:"]},{"cell_type":"code","metadata":{"id":"CQtpyeMhmu8d","colab_type":"code","colab":{}},"source":["date + np.arange(12)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G2WRTo-VmvBj","colab_type":"text"},"source":["Because of the uniform type in NumPy ``datetime64`` arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python's ``datetime`` objects, especially as arrays get large.\n","\n","One detail of the ``datetime64`` and ``timedelta64`` objects is that they are built on a *fundamental time unit*.\n","Because the ``datetime64`` object is limited to 64-bit precision, the range of encodable times is $2^{64}$ times this fundamental unit.\n","In other words, ``datetime64`` imposes a trade-off between *time resolution* and *maximum time span*.\n","\n","For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of $2^{64}$ nanoseconds, or just under 600 years.\n","NumPy will infer the desired unit from the input; for example, here is a day-based datetime:"]},{"cell_type":"code","metadata":{"id":"4b9YbhuWmvGA","colab_type":"code","colab":{}},"source":["np.datetime64('2015-07-04')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qiFQl3BMmvKL","colab_type":"text"},"source":["Here is a minute-based datetime:"]},{"cell_type":"code","metadata":{"id":"uXYMqQdXmvOn","colab_type":"code","colab":{}},"source":["np.datetime64('2015-07-04 12:00')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iV5tNvrmmvTx","colab_type":"text"},"source":["Notice that the time zone is automatically set to the local time on the computer executing the code.\n","You can force any desired fundamental unit using one of many format codes; for example, here we'll force a nanosecond-based time:"]},{"cell_type":"code","metadata":{"id":"KJJ1T45amvXB","colab_type":"code","colab":{}},"source":["np.datetime64('2015-07-04 12:59:59.50', 'ns')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-R22MBGvnwiG","colab_type":"text"},"source":["The following table, drawn from the [NumPy datetime64 documentation](http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html), lists the available format codes along with the relative and absolute timespans that they can encode:\n","\n","|Code    | Meaning     | Time span (relative) | Time span (absolute)   |\n","|--------|-------------|----------------------|------------------------|\n","| ``Y``  | Year\t       | ± 9.2e18 years       | [9.2e18 BC, 9.2e18 AD] |\n","| ``M``  | Month       | ± 7.6e17 years       | [7.6e17 BC, 7.6e17 AD] |\n","| ``W``  | Week\t       | ± 1.7e17 years       | [1.7e17 BC, 1.7e17 AD] |\n","| ``D``  | Day         | ± 2.5e16 years       | [2.5e16 BC, 2.5e16 AD] |\n","| ``h``  | Hour        | ± 1.0e15 years       | [1.0e15 BC, 1.0e15 AD] |\n","| ``m``  | Minute      | ± 1.7e13 years       | [1.7e13 BC, 1.7e13 AD] |\n","| ``s``  | Second      | ± 2.9e12 years       | [ 2.9e9 BC, 2.9e9 AD]  |\n","| ``ms`` | Millisecond | ± 2.9e9 years        | [ 2.9e6 BC, 2.9e6 AD]  |\n","| ``us`` | Microsecond | ± 2.9e6 years        | [290301 BC, 294241 AD] |\n","| ``ns`` | Nanosecond  | ± 292 years          | [ 1678 AD, 2262 AD]    |\n","| ``ps`` | Picosecond  | ± 106 days           | [ 1969 AD, 1970 AD]    |\n","| ``fs`` | Femtosecond | ± 2.6 hours          | [ 1969 AD, 1970 AD]    |\n","| ``as`` | Attosecond  | ± 9.2 seconds        | [ 1969 AD, 1970 AD]    |"]},{"cell_type":"markdown","metadata":{"id":"DwbjzqS4nwnW","colab_type":"text"},"source":["### Dates and times in pandas: best of both worlds\n","\n","Pandas builds upon all the tools just discussed to provide a ``Timestamp`` object, which combines the ease-of-use of ``datetime`` and ``dateutil`` with the efficient storage and vectorized interface of ``numpy.datetime64``.\n","From a group of these ``Timestamp`` objects, Pandas can construct a ``DatetimeIndex`` that can be used to index data in a ``Series`` or ``DataFrame``; we'll see many examples of this below.\n","\n","For example, we can use Pandas tools to repeat the demonstration from above.\n","We can parse a flexibly formatted string date, and use format codes to output the day of the week:"]},{"cell_type":"code","metadata":{"id":"SoacLbKCnwsc","colab_type":"code","colab":{}},"source":["date = pd.to_datetime(\"4th of July, 2015\")\n","date"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_tUCT1PnwxG","colab_type":"code","colab":{}},"source":["date.strftime('%A')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OEJCttRPoBjE","colab_type":"text"},"source":["Additionally, we can do NumPy-style vectorized operations directly on this same object:"]},{"cell_type":"code","metadata":{"id":"3CzfQ-A9oBnu","colab_type":"code","colab":{}},"source":["date + pd.to_timedelta(np.arange(12), 'D')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwVuCs6FoBsj","colab_type":"text"},"source":["In the next section, we will take a closer look at manipulating time series data with the tools provided by Pandas."]},{"cell_type":"markdown","metadata":{"id":"_4xnoS36oBwo","colab_type":"text"},"source":["## Pandas Time Series: Indexing by Time\n","\n","Where the Pandas time series tools really become useful is when you begin to *index data by timestamps*.\n","For example, we can construct a ``Series`` object that has time indexed data:"]},{"cell_type":"code","metadata":{"id":"4waw9H9poB1Q","colab_type":"code","colab":{}},"source":["index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n","                          '2015-07-04', '2015-08-04'])\n","data = pd.Series([0, 1, 2, 3], index=index)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kHrWQp8KoB5C","colab_type":"text"},"source":["Now that we have this data in a ``Series``, we can make use of any of the ``Series`` indexing patterns we discussed in previous sections, passing values that can be coerced into dates:"]},{"cell_type":"code","metadata":{"id":"XC3Bhn88oB-l","colab_type":"code","colab":{}},"source":["data['2014-07-04':'2015-07-04']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRMwziitoCCE","colab_type":"text"},"source":["There are additional special date-only indexing operations, such as passing a year to obtain a slice of all data from that year:"]},{"cell_type":"code","metadata":{"id":"WYfReQYkoCHk","colab_type":"code","colab":{}},"source":["data['2015']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xgh6PCHipr2L","colab_type":"text"},"source":["Later, we will see additional examples of the convenience of dates-as-indices.\n","But first, a closer look at the available time series data structures.\n","\n","## Pandas Time Series Data Structures\n","\n","This section will introduce the fundamental Pandas data structures for working with time series data:\n","\n","- For *time stamps*, Pandas provides the ``Timestamp`` type. As mentioned before, it is essentially a replacement for Python's native ``datetime``, but is based on the more efficient ``numpy.datetime64`` data type. The associated Index structure is ``DatetimeIndex``.\n","- For *time Periods*, Pandas provides the ``Period`` type. This encodes a fixed-frequency interval based on ``numpy.datetime64``. The associated index structure is ``PeriodIndex``.\n","- For *time deltas* or *durations*, Pandas provides the ``Timedelta`` type. ``Timedelta`` is a more efficient replacement for Python's native ``datetime.timedelta`` type, and is based on ``numpy.timedelta64``. The associated index structure is ``TimedeltaIndex``.\n","\n","The most fundamental of these date/time objects are the ``Timestamp`` and ``DatetimeIndex`` objects.\n","While these class objects can be invoked directly, it is more common to use the ``pd.to_datetime()`` function, which can parse a wide variety of formats.\n","Passing a single date to ``pd.to_datetime()`` yields a ``Timestamp``; passing a series of dates by default yields a ``DatetimeIndex``:"]},{"cell_type":"code","metadata":{"id":"c7tn9HbNpr7A","colab_type":"code","colab":{}},"source":["dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\n","                       '2015-Jul-6', '07-07-2015', '20150708'])\n","dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdduMN-Spr_-","colab_type":"text"},"source":["Any ``DatetimeIndex`` can be converted to a ``PeriodIndex`` with the ``to_period()`` function with the addition of a frequency code; here we'll use ``'D'`` to indicate daily frequency:"]},{"cell_type":"code","metadata":{"id":"vOjEnAgJpsFe","colab_type":"code","colab":{}},"source":["dates.to_period('D')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAJ7gK-_psK4","colab_type":"text"},"source":["A ``TimedeltaIndex`` is created, for example, when a date is subtracted from another:"]},{"cell_type":"code","metadata":{"id":"4DrtMwbVoCLI","colab_type":"code","colab":{}},"source":["dates - dates[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMe0pAdWoCO7","colab_type":"text"},"source":["### Regular sequences: ``pd.date_range()``\n","\n","To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: ``pd.date_range()`` for timestamps, ``pd.period_range()`` for periods, and ``pd.timedelta_range()`` for time deltas.\n","We've seen that Python's ``range()`` and NumPy's ``np.arange()`` turn a startpoint, endpoint, and optional stepsize into a sequence.\n","Similarly, ``pd.date_range()`` accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates.\n","By default, the frequency is one day:"]},{"cell_type":"code","metadata":{"id":"SqfRDE2xsmkd","colab_type":"code","colab":{}},"source":["pd.date_range('2015-07-03', '2015-07-10')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDosiuSTsmp9","colab_type":"text"},"source":["Alternatively, the date range can be specified not with a start and endpoint, but with a startpoint and a number of periods:"]},{"cell_type":"code","metadata":{"id":"6yuc61SPsmvT","colab_type":"code","colab":{}},"source":["pd.date_range('2015-07-03', periods=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDk9ITvRsm1_","colab_type":"text"},"source":["The spacing can be modified by altering the ``freq`` argument, which defaults to ``D``.\n","For example, here we will construct a range of hourly timestamps:"]},{"cell_type":"code","metadata":{"id":"DC8Pf8IGsm5m","colab_type":"code","colab":{}},"source":["pd.date_range('2015-07-03', periods=8, freq='H')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuCuyHxqsm-T","colab_type":"text"},"source":["To create regular sequences of ``Period`` or ``Timedelta`` values, the very similar ``pd.period_range()`` and ``pd.timedelta_range()`` functions are useful.\n","Here are some monthly periods:"]},{"cell_type":"code","metadata":{"id":"MLWpUcv3snDg","colab_type":"code","colab":{}},"source":["pd.period_range('2015-07', periods=8, freq='M')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UxO_chyksy2E","colab_type":"text"},"source":["And a sequence of durations increasing by an hour:"]},{"cell_type":"code","metadata":{"id":"3KNHh_imsy67","colab_type":"code","colab":{}},"source":["pd.timedelta_range(0, periods=10, freq='H')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LFLH_qqFszAP","colab_type":"text"},"source":["All of these require an understanding of Pandas frequency codes, which we'll summarize in the next section.\n","\n","## Frequencies and Offsets\n","\n","Fundamental to these Pandas time series tools is the concept of a frequency or date offset.\n","Just as we saw the ``D`` (day) and ``H`` (hour) codes above, we can use such codes to specify any desired frequency spacing.\n","The following table summarizes the main codes available:\n","\n","| Code   | Description         | Code   | Description          |\n","|--------|---------------------|--------|----------------------|\n","| ``D``  | Calendar day        | ``B``  | Business day         |\n","| ``W``  | Weekly              |        |                      |\n","| ``M``  | Month end           | ``BM`` | Business month end   |\n","| ``Q``  | Quarter end         | ``BQ`` | Business quarter end |\n","| ``A``  | Year end            | ``BA`` | Business year end    |\n","| ``H``  | Hours               | ``BH`` | Business hours       |\n","| ``T``  | Minutes             |        |                      |\n","| ``S``  | Seconds             |        |                      |\n","| ``L``  | Milliseonds         |        |                      |\n","| ``U``  | Microseconds        |        |                      |\n","| ``N``  | nanoseconds         |        |                      |\n","\n","\n","The monthly, quarterly, and annual frequencies are all marked at the end of the specified period.\n","By adding an ``S`` suffix to any of these, they instead will be marked at the beginning:\n","\n","| Code    | Description            || Code    | Description            |\n","|---------|------------------------||---------|------------------------|\n","| ``MS``  | Month start            ||``BMS``  | Business month start   |\n","| ``QS``  | Quarter start          ||``BQS``  | Business quarter start |\n","| ``AS``  | Year start             ||``BAS``  | Business year start    |\n","\n","Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix:\n","\n","- ``Q-JAN``, ``BQ-FEB``, ``QS-MAR``, ``BQS-APR``, etc.\n","- ``A-JAN``, ``BA-FEB``, ``AS-MAR``, ``BAS-APR``, etc.\n","\n","In the same way, the split-point of the weekly frequency can be modified by adding a three-letter weekday code:\n","\n","- ``W-SUN``, ``W-MON``, ``W-TUE``, ``W-WED``, etc.\n","\n","On top of this, codes can be combined with numbers to specify other frequencies.\n","For example, for a frequency of 2 hours 30 minutes, we can combine the hour (``H``) and minute (``T``) codes as follows:\n"]},{"cell_type":"code","metadata":{"id":"Z6Co8j1aszFw","colab_type":"code","colab":{}},"source":["pd.timedelta_range(0, periods=9, freq=\"2H30T\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jatDA0xbszLo","colab_type":"text"},"source":["All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the ``pd.tseries.offsets`` module.\n","For example, we can create a business day offset directly as follows:"]},{"cell_type":"code","metadata":{"id":"Syc1PMFHszQS","colab_type":"code","colab":{}},"source":["from pandas.tseries.offsets import BDay\n","pd.date_range('2015-07-01', periods=5, freq=BDay())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6a9PSgNiszWF","colab_type":"text"},"source":["For more discussion of the use of frequencies and offsets, see the [\"DateOffset\" section](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects) of the Pandas documentation.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_Yncy5cdszad","colab_type":"text"},"source":["# High-Performance Pandas: eval() and query()\n","\n","As we've already seen in previous lectures, the power of the PyData stack is built upon the ability of NumPy and Pandas to push basic operations into C via an intuitive syntax: examples are vectorized/broadcasted operations in NumPy, and grouping-type operations in Pandas.\n","While these abstractions are efficient and effective for many common use cases, they often rely on the creation of temporary intermediate objects, which can cause undue overhead in computational time and memory use.\n","\n","Pandas includes some experimental tools that allow us to directly access C-speed operations without costly allocation of intermediate arrays.\n","These are the ``eval()`` and ``query()`` functions, which rely on the [Numexpr](https://github.com/pydata/numexpr) package.\n","In this notebook we will walk through their use and give some rules-of-thumb about when you might think about using them.\n","\n","## Motivating ``query()`` and ``eval()``: Compound Expressions\n","\n","We've seen previously that NumPy and Pandas support fast vectorized operations; for example, when adding the elements of two arrays:\n"]},{"cell_type":"code","metadata":{"id":"xKLnFef0snN8","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(42)\n","x = rng.rand(1000000)\n","y = rng.rand(1000000)\n","%timeit x + y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLse8OMhtn8K","colab_type":"text"},"source":["As discussed earlier, this is much faster than doing the addition via a Python loop or comprehension:"]},{"cell_type":"code","metadata":{"id":"ameOl1lytoAk","colab_type":"code","colab":{}},"source":["%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVY8jou7toFu","colab_type":"text"},"source":["But this abstraction can become less efficient when computing compound expressions.\n","For example, consider the following expression:"]},{"cell_type":"code","metadata":{"id":"75tKO5JotoKw","colab_type":"code","colab":{}},"source":["mask = (x > 0.5) & (y < 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sy0Gt-WZtoPC","colab_type":"text"},"source":["Because NumPy evaluates each subexpression, this is roughly equivalent to the following:"]},{"cell_type":"code","metadata":{"id":"BFbIdvEJtoT5","colab_type":"code","colab":{}},"source":["tmp1 = (x > 0.5)\n","tmp2 = (y < 0.5)\n","mask = tmp1 & tmp2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"abn1mnSotoYe","colab_type":"text"},"source":["In other words, *every intermediate step is explicitly allocated in memory*. If the ``x`` and ``y`` arrays are very large, this can lead to significant memory and computational overhead.\n","The Numexpr library gives you the ability to compute this type of compound expression element by element, without the need to allocate full intermediate arrays.\n","The [Numexpr documentation](https://github.com/pydata/numexpr) has more details, but for the time being it is sufficient to say that the library accepts a *string* giving the NumPy-style expression you'd like to compute:"]},{"cell_type":"code","metadata":{"id":"zmFwkWnjtodK","colab_type":"code","colab":{}},"source":["import numexpr\n","mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)')\n","np.allclose(mask, mask_numexpr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3p5S_ugMvvar","colab_type":"text"},"source":["The benefit here is that Numexpr evaluates the expression in a way that does not use full-sized temporary arrays, and thus can be much more efficient than NumPy, especially for large arrays.\n","The Pandas ``eval()`` and ``query()`` tools that we will discuss here are conceptually similar, and depend on the Numexpr package."]},{"cell_type":"markdown","metadata":{"id":"FrUsjzYBvvfJ","colab_type":"text"},"source":["## ``pandas.eval()`` for Efficient Operations\n","\n","The ``eval()`` function in Pandas uses string expressions to efficiently compute operations using ``DataFrame``s.\n","For example, consider the following ``DataFrame``s:"]},{"cell_type":"code","metadata":{"id":"UWbIasHFvvjh","colab_type":"code","colab":{}},"source":["nrows, ncols = 100000, 100\n","rng = np.random.RandomState(42)\n","df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols))\n","                      for i in range(4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1Gngvskvvnp","colab_type":"text"},"source":["To compute the sum of all four ``DataFrame``s using the typical Pandas approach, we can just write the sum:"]},{"cell_type":"code","metadata":{"id":"L4Tw7Gj7vvsR","colab_type":"code","colab":{}},"source":["%timeit df1 + df2 + df3 + df4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwcH9IC9toh4","colab_type":"text"},"source":["The same result can be computed via ``pd.eval`` by constructing the expression as a string:"]},{"cell_type":"code","metadata":{"id":"Tw6Hh34ov6XI","colab_type":"code","colab":{}},"source":["%timeit pd.eval('df1 + df2 + df3 + df4')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXCU_u8Sv6cr","colab_type":"text"},"source":["The ``eval()`` version of this expression is about 50% faster (and uses much less memory), while giving the same result:"]},{"cell_type":"code","metadata":{"id":"sBpCh1aKv6hi","colab_type":"code","colab":{}},"source":["np.allclose(df1 + df2 + df3 + df4,\n","            pd.eval('df1 + df2 + df3 + df4'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hd-syEHXv6mr","colab_type":"text"},"source":["### Operations supported by ``pd.eval()``\n","\n","As of Pandas v0.16, ``pd.eval()`` supports a wide range of operations.\n","To demonstrate these, we'll use the following integer ``DataFrame``s:"]},{"cell_type":"code","metadata":{"id":"rz5pa_Cav6sd","colab_type":"code","colab":{}},"source":["df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3)))\n","                           for i in range(5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLyAkcHtv6x4","colab_type":"text"},"source":["#### Arithmetic operators\n","``pd.eval()`` supports all arithmetic operators. For example:"]},{"cell_type":"code","metadata":{"id":"1HCp-UmRv63Y","colab_type":"code","colab":{}},"source":["result1 = -df1 * df2 / (df3 + df4) - df5\n","result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8D-E2godv683","colab_type":"text"},"source":["#### Comparison operators\n","``pd.eval()`` supports all comparison operators, including chained expressions:"]},{"cell_type":"code","metadata":{"id":"aguApOcNv7Bl","colab_type":"code","colab":{}},"source":["result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4)\n","result2 = pd.eval('df1 < df2 <= df3 != df4')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"De8-tnCgwKrF","colab_type":"text"},"source":["#### Bitwise operators\n","``pd.eval()`` supports the ``&`` and ``|`` bitwise operators:"]},{"cell_type":"code","metadata":{"id":"E9VjZkshwKw0","colab_type":"code","colab":{}},"source":["result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)\n","result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5kRGse1wK11","colab_type":"text"},"source":["In addition, it supports the use of the literal ``and`` and ``or`` in Boolean expressions:"]},{"cell_type":"code","metadata":{"id":"1948yuUWwK7W","colab_type":"code","colab":{}},"source":["result3 = pd.eval('(df1 < 0.5) and (df2 < 0.5) or (df3 < df4)')\n","np.allclose(result1, result3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ags6DPn6wLAu","colab_type":"text"},"source":["#### Object attributes and indices\n","\n","``pd.eval()`` supports access to object attributes via the ``obj.attr`` syntax, and indexes via the ``obj[index]`` syntax:"]},{"cell_type":"code","metadata":{"id":"jjS7Jb9XwLGv","colab_type":"code","colab":{}},"source":["result1 = df2.T[0] + df3.iloc[1]\n","result2 = pd.eval('df2.T[0] + df3.iloc[1]')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkrUkHb9wLL1","colab_type":"text"},"source":["#### Other operations\n","Other operations such as function calls, conditional statements, loops, and other more involved constructs are currently *not* implemented in ``pd.eval()``.\n","If you'd like to execute these more complicated types of expressions, you can use the Numexpr library itself.\n","\n","## ``DataFrame.eval()`` for Column-Wise Operations\n","\n","Just as Pandas has a top-level ``pd.eval()`` function, ``DataFrame``s have an ``eval()`` method that works in similar ways.\n","The benefit of the ``eval()`` method is that columns can be referred to *by name*.\n","We'll use this labeled array as an example:"]},{"cell_type":"code","metadata":{"id":"g0zQuf5rwLQ1","colab_type":"code","colab":{}},"source":["df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C'])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shnnPqrJwLWE","colab_type":"text"},"source":["Using ``pd.eval()`` as above, we can compute expressions with the three columns like this:"]},{"cell_type":"code","metadata":{"id":"fVdC5ZGIwLbN","colab_type":"code","colab":{}},"source":["result1 = (df['A'] + df['B']) / (df['C'] - 1)\n","result2 = pd.eval(\"(df.A + df.B) / (df.C - 1)\")\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYLZ4NRywLgE","colab_type":"text"},"source":["The ``DataFrame.eval()`` method allows much more succinct evaluation of expressions with the columns:"]},{"cell_type":"code","metadata":{"id":"h83W1ujnwged","colab_type":"code","colab":{}},"source":["result3 = df.eval('(A + B) / (C - 1)')\n","np.allclose(result1, result3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61r-_LKXwgjt","colab_type":"text"},"source":["Notice here that we treat *column names as variables* within the evaluated expression, and the result is what we would wish.\n","\n","### Assignment in DataFrame.eval()\n","\n","In addition to the options just discussed, ``DataFrame.eval()``  also allows assignment to any column.\n","Let's use the ``DataFrame`` from before, which has columns ``'A'``, ``'B'``, and ``'C'``:"]},{"cell_type":"code","metadata":{"id":"9WLQPVE5wgom","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmFbARAmwgt4","colab_type":"text"},"source":["We can use ``df.eval()`` to create a new column ``'D'`` and assign to it a value computed from the other columns:"]},{"cell_type":"code","metadata":{"id":"K39aeXPIwxBU","colab_type":"code","colab":{}},"source":["df.eval('D = (A + B) / C', inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rg20r1-AwxGR","colab_type":"text"},"source":["In the same way, any existing column can be modified:"]},{"cell_type":"code","metadata":{"id":"B-6oPsMmwxLg","colab_type":"code","colab":{}},"source":["df.eval('D = (A - B) / C', inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkF6e1Y4wxQV","colab_type":"text"},"source":["### Local variables in DataFrame.eval()\n","\n","The ``DataFrame.eval()`` method supports an additional syntax that lets it work with local Python variables.\n","Consider the following:"]},{"cell_type":"code","metadata":{"id":"jE9ueoJGwxVg","colab_type":"code","colab":{}},"source":["column_mean = df.mean(1)\n","result1 = df['A'] + column_mean\n","result2 = df.eval('A + @column_mean')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rsYZqPLdwxaf","colab_type":"text"},"source":["The ``@`` character here marks a *variable name* rather than a *column name*, and lets you efficiently evaluate expressions involving the two \"namespaces\": the namespace of columns, and the namespace of Python objects.\n","Notice that this ``@`` character is only supported by the ``DataFrame.eval()`` *method*, not by the ``pandas.eval()`` *function*, because the ``pandas.eval()`` function only has access to the one (Python) namespace."]},{"cell_type":"markdown","metadata":{"id":"y_uOvtlFw58Y","colab_type":"text"},"source":["## DataFrame.query() Method\n","\n","The ``DataFrame`` has another method based on evaluated strings, called the ``query()`` method.\n","Consider the following:"]},{"cell_type":"code","metadata":{"id":"wYbR7UQgw6Ba","colab_type":"code","colab":{}},"source":["result1 = df[(df.A < 0.5) & (df.B < 0.5)]\n","result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BbaLDl5Rw6Gl","colab_type":"text"},"source":["As with the example used in our discussion of ``DataFrame.eval()``, this is an expression involving columns of the ``DataFrame``.\n","It cannot be expressed using the ``DataFrame.eval()`` syntax, however!\n","Instead, for this type of filtering operation, you can use the ``query()`` method:"]},{"cell_type":"code","metadata":{"id":"xT5DkeIJw6LZ","colab_type":"code","colab":{}},"source":["result2 = df.query('A < 0.5 and B < 0.5')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whQwP8GNw6QO","colab_type":"text"},"source":["In addition to being a more efficient computation, compared to the masking expression this is much easier to read and understand.\n","Note that the ``query()`` method also accepts the ``@`` flag to mark local variables:"]},{"cell_type":"code","metadata":{"id":"kEzdbGqZw6VC","colab_type":"code","colab":{}},"source":["Cmean = df['C'].mean()\n","result1 = df[(df.A < Cmean) & (df.B < Cmean)]\n","result2 = df.query('A < @Cmean and B < @Cmean')\n","np.allclose(result1, result2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BURnyvZ7wxe_","colab_type":"text"},"source":["In practice, the difference in computation time between the traditional methods and the ``eval``/``query`` method is usually not significant–if anything, the traditional method is faster for smaller arrays!\n","The benefit of ``eval``/``query`` is mainly in the saved memory, and the sometimes cleaner syntax they offer."]},{"cell_type":"markdown","metadata":{"id":"hsmZEjyH3QQa","colab_type":"text"},"source":["# Creating Copies "]},{"cell_type":"markdown","metadata":{"id":"TM7BRBBZ-PTc","colab_type":"text"},"source":["In Pandas, indexing a DataFrame returns a reference to the initial DataFrame. Thus, changing the subset will change the initial DataFrame, similar to NumPy. Thus, you'd want to use the copy if you want to make sure the initial DataFrame shouldn't change. Consider the following code:"]},{"cell_type":"code","metadata":{"id":"CY-7OAAG3QqI","colab_type":"code","colab":{}},"source":["df = pd.DataFrame({'x': [1,2]})\n","df_sub = df[0:1]\n","df_sub.x = -1\n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMODECPDB2i6","colab_type":"text"},"source":["In contrast, the following leaves ``df`` unchanged:"]},{"cell_type":"code","metadata":{"id":"tbWlZssq3edn","colab_type":"code","colab":{}},"source":["df = pd.DataFrame({'x': [1,2]})\n","df_sub_copy = df[0:1].copy()\n","df_sub_copy.x = -1\n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPs5ycB03euH","colab_type":"text"},"source":["We have covered the main aspects of the pandas package and it is not expected that you will remember all this at once. There are cheat sheets [[1]](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf), [[2]](https://drive.google.com/file/d/1UHK8wtWbADvHKXFC937IS6MTnlSZC_zB/view) that can help you remember the most commonly used functions in practice."]}]}